"""
CSCC11 - Introduction to Machine Learning, Fall 2020, Assignment 4 - Dimensionality Reduction
B. Chan, S. Wei, D. Fleet
"""

Answer The Following Questions:

Understanding Eigenvalues:
1. How do the eigenvalues relate to the diagonal variances used in generating the data?

> The diagonal variances used in generating the data are 4^2, 3^2, I,e, 16, 4, which resemble the 1st 2 eigenvalues generated by pca.py. These diagonal variances are those of an independent Gaussian in a subspace of dimension 2 and our eigenvalues from pca.py are the variances shown by the Principal Components that we are using to reduce the dimensionality (to a 2 dimensional subspace from a 10 dimensional Data space)

2. How does the additive noise affect the curves?

> If there was no noise in generated data, then we would be able to perfectly capture 100% of the variance in data using a few (2) principal components as they could be clearly projected on to the desired subspace with minimal (~0) residual error. Without the noise, the 'Eigenvalues' plot would decrease steeply to 0 by the 3rd dimension point (on X axis) and the 'Fractions of Total Variance' plot would steeply increase to its maximum possible value of 1 at the 2nd dimension which shows that we could perfectly represent 100% of variance in original data by 2 principal components. The noise is the reason why the curves (and the PCA algorithm) still have to account for reduction of perpendicular error during reduction/projection of data into a lower dimensional subspace.

3. Based on the plots, can you hypothesize the ways to choose the number of subspace dimensions?

> We can choose the number of subspace dimensions based on any of the 2 plots. We can look at the 'Eigenvalues' plot and see that the curve becomes is decreasing in a steep fashion and around after the 2nd/3rd dimension, we can see that area beneath the curve becomes really low, which implies the perpendicular residual error becomes really low. Also, looking at the 'Fractions of Total Variance' graph, we can see that curve increases steeply till we reach the 2nd dimension and then the slope starts becoming smaller and smaller, i,e, curve becomes flatter, We can see that an upward of 90% of variance is explained by the 1st 2 principal components so we can choose the first 2 (if we're looking to capture say 90% of variance) principal components.


PCA on document data:
1. How big is the covariance matrix used in the PCA algorithm?

> The covariance matrix used in the PCA algorithm has size D x D where D is the number of dimensions of our N D-dimensional observation data.

2. How long does PCA algorithm take?

> The PCA algorithm takes ~107 (107.58 when I ran it) seconds or about 1.5 - 1.7 minutes to run.

3. Do the points from different classes look reasonably separable in this space?

> Yes, I believe the points from different classes look reasonably separable in this class.


EM-PCA v.s. PCA:
1. After running visualize_documents.py, compare the parameters you've estimated using PCA and EM-PCA. Are they identical and if not, how do they differ?

> Based on the plot, the classes still look reasonable separable, but it can be seen the plot points have basically been flipped in sign (EM-PCA plot shows a reflected plot of the PCA one), i,e, The EM-PCA estimated subspace basis has its signs flipped compared to the PCA estimated subspace basis, however, the subspace basis estimated by both PCA and EM-PCA are similar in magnitude.

2. Which algorithm takes more space?

> PCA algorithm takes more space than the EM-PCA algorithm.


3. How long does EM-PCA algorithm take to run compared to PCA?

> EM-PCA algorithm takes about 3 seconds (2.95s exactly when I ran it) to run whereas the PCA algorithm took about ~1.5 minutes to run. Therefore, EM-PCA algorithm run-time was much faster than PCA algorithm.


