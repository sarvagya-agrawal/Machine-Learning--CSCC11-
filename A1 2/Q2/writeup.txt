CSCC11 - Introduction to Machine Learning, Fall 2020, Assignment 1
B. Chan, S. Wei, D. Fleet

===========================================================

 COMPLETE THIS TEXT BOX:

 Student Name: Sarvagya Agrawal
 Student number: 1004574819
 UtorID: agrawa81

 I hereby certify that the work contained here is my own


  __Sarvagya Agrawal______________
 (sign with your name)

===========================================================

For the following questions, please answer concisely in bullet points.

Q1: Dataset Size
- If I increase the size of the training set, what can we expect about the model's
  training error? What about the test error?

> Increasing size of training data can have several different kinds of effects on training and test error, these effects are also dependant on the nature of the model in question and on the kind of data added. Small training datasets can lead to the modelling of noise in data. We dont want this as this can lead to overfitting. If we model the noise in our training set (when dataset is too small), we also end up accounting for that noise in test data which leads to higher test error. Therefore, we can see that as training dataset sizes increase, training error increases as now that there are so many more data points than parameters of the model, the solution (weights/parameters) becomes less and less unique making it harder and harder to fit training data perfectly (thus, increases training error); and as training dataset sizes increase, we can see a reduction in test error as a large training dataset teaches the model to account for different kinds and variety of data and to not account for noise in data, and therefore, allows better estimation/prediction of new test data, and hence, a lower test error.

- If I increase the size of the test set, what can we expect the about the model's
  training error? What about the test error?

> Increasing size of test set has no effect on training error as test data comes after the training data and by the time we test the test data, we have already modelled for the training data and therefore that regression stays the same and therefore that training error also stays the same. Unaffected by test data.
> Increasing test data size may cause the test error to stay the same, i.e. when the model continues to give predictions in the expected fashion. It may also cause the test error to increase when the model is asked to predict an output for an input that may be very different or quite farther away from the range/spectrum of inputs fed to the model in the training data. In such a situation, the model may not be able to make a good prediction and that can increase test error.

- How much data should I try to obtain if I want to build a good model?

> Our goal is to minimize test error. That's what makes a good model. To minimize test error, we need to account for all different kinds of inputs that model could probably be fed as test data, i,e. We need a large enough dataset to cover all possible cases that may  appear in our test dataset. All in all, we need to have a training dataset that very closely resembles the actual data set in terms of its main properties and a large enough dataset to cancel out the noise, to avoid overfitting.

Q2: Model Complexity
- If the model is too simple, what can we conclude about the training and test errors?

>Looking at the model complexity vs. Error graph for all 3 datasets, we notice some generalizable patterns:
>> When training dataset size is large, we see that both training and test errors are high. This can be explained by the fact that higher number of parameters are required to model large number of data-points.
>> When training dataset size is small, we see that the training and test errors are still high but relatively lower to the errors observed with a large training dataset size. We can also see that the training error is relatively lower than test error. This can be explained by the idea that when dataset size is small, it is easier to model the data with lower number of parameters but still training error exists if the model is way too simple. However, with test data, test error stays high as the simple model will not be able to give good predictions for test inputs which really differ from the inputs given in the small training dataset. 

- If the model is too complex, what can we conclude about the training and test errors?

> Looking at the model complexity vs. Error graph for all 3 datasets, we notice some generalizable patterns:
>> When model becomes more complex, training error becomes lower. This can be explained by the idea that as the model becomes more complex, i.e. the degree of the polynomial of the model increases, the model becomes more expressive and it starts to become more fluctuating and can now account for more training data points more accurately. Basically, a more complex model is able to model the training data better.
>> Test error also starts to decrease with increasing model complexity but however, there comes a point, after which if the model complexity is increased, the test error starts increasing again. This is because, as the model becomes more complex and starts fitting the training data extremely accurately, it starts accounting for and modelling the noise in the data, this can lead to overfitting which basically means that our model is so good at fitting the training data, that a little bit of noise in the test data can cause it to give off predictions, causing an increase in test error. Essentially, too complex a model increases test error. 

- Which (degree) model gives the best performance? How did you find out?

> For Dataset 1, degree of 4 makes the model perform the best. We can see that test error reaches a minimum value when model complexity (polynomial degree) is 4 given a large training dataset which shows that a 4 degree polynomial models the test data with least error.
>  For Dataset 2, degree of 6 makes the model perform the best. We can see that test error reaches a minimum value when model complexity (polynomial degree) is 6 given a large training dataset which shows that a 6 degree polynomial models the test data with least error.
>  For Dataset 3, degree of 5 makes the model perform the best. We can see that test error reaches a minimum value when model complexity (polynomial degree) is 5 given a large training dataset which shows that a 5 degree polynomial models the test data with least error.

- What degree of polynomial do you think was used to generate the data?

> I believe a 4th degree polynomial was used to generate this data as looking at all the model complexity vs. Error graphs, we can see that the training error is roughly 0 when modelling using a 4 degree polynomial. That tells us that a 4 degree polynomial could have been used to generate this data.

Q3: Regularization
- What does regularization do to the weights? Note: You may want to look at the weight values.

> When model is too complex, we want to make it less complex by making the model smoother. In order to do that, we use regularization to decrease value of weights when they are too high, and when they are making the model too complex. Using regularization to decrease these weights values is a way of making the model simpler, smoother and also a way of avoiding overfitting. This is done by inclusion of a regularization parameter (lambda), which is also known as the penalty term (penalty for increasing complexity of model/increasing scalar values of weights). 

- If we set lambda (l2_coef) to 0, what do we get?

> When lambda is set to 0, our objective function (loss function) basically becomes equal to our data term and there is no smoothness/regularization term anymore. Therefore, when lambda is 0, we get the usual regression model and the usual results we would expect when we are performing regression without regularization.

- What does increasing lambda (l2_coef) do to our loss function?

> Increasing lambda (l2_coef) causes our test error to decrease. The test error basically refers to our loss function. As our lambda increases, our objective/loss function becomes more and more minimized (which is what we want) and the weight values (parameter values) become smaller which supports the idea that the model becomes more and more smooth as lambda and subsequently, the smoothness term increases in value. Minimizing the loss function is the crux of our study and increasing lambda helps us achieve that.


